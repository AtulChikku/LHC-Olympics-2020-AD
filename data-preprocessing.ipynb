{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13415671,"sourceType":"datasetVersion","datasetId":8514581},{"sourceId":13484103,"sourceType":"datasetVersion","datasetId":8425451}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install hdf5plugin","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset , DataLoader , TensorDataset\nfrom torch.utils.data import random_split\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport h5py\nimport os\nimport hdf5plugin\nfrom sklearn.preprocessing import StandardScaler\nimport gc\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:40:10.765357Z","iopub.execute_input":"2025-10-18T10:40:10.765670Z","iopub.status.idle":"2025-10-18T10:40:16.814211Z","shell.execute_reply.started":"2025-10-18T10:40:10.765644Z","shell.execute_reply":"2025-10-18T10:40:16.813233Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"torch.manual_seed(50)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:40:16.815526Z","iopub.execute_input":"2025-10-18T10:40:16.816035Z","iopub.status.idle":"2025-10-18T10:40:16.909557Z","shell.execute_reply.started":"2025-10-18T10:40:16.816013Z","shell.execute_reply":"2025-10-18T10:40:16.908630Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## **Inspecting Hierarchy Of Data**","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/lhc-olympics-2020-ad-r-and-d/events_anomalydetection_v2.h5'\ngroup_key = 'df'\n\nprint(\"--- Inspecting HDF5 File Structure ---\")\nwith h5py.File(file_path, 'r') as f:\n    # Check if the key points to a group\n    if isinstance(f[group_key], h5py.Group):\n        group = f[group_key]\n        inner_keys = list(group.keys())\n        print(f\"Keys found inside group '{group_key}': {inner_keys}\")\n        \n        # Print the shape of each dataset within the group\n        for key in inner_keys:\n            print(f\"  -> Dataset '{key}' has shape: {group[key].shape}\")\n            \nprint(\"--- Inspection Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T11:14:26.106355Z","iopub.execute_input":"2025-10-17T11:14:26.106946Z","iopub.status.idle":"2025-10-17T11:14:26.153453Z","shell.execute_reply.started":"2025-10-17T11:14:26.106924Z","shell.execute_reply":"2025-10-17T11:14:26.152798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Creating Jet Images**","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/lhc-olympics-2020-ad-r-and-d/events_anomalydetection_v2.h5'\ndataset_path = 'df/block0_values'\noutput_path = 'processed_jet_images.h5'\nchunk_size = 50000 \nimage_shape = (50, 50)\nimage_bins = [50, 50]\nimage_range = [[-5, 5], [-np.pi, np.pi]] \n\nwith h5py.File(file_path, 'r') as f_in:\n    dataset = f_in[dataset_path]\n    num_events = dataset.shape[0]\n\n    with h5py.File(output_path, 'w') as f_out:\n        image_dataset = f_out.create_dataset(\n            'jet_images',\n            shape=(0, image_shape[0], image_shape[1]),\n            maxshape=(None, image_shape[0], image_shape[1]),\n            dtype='float32',\n            chunks=True\n        )\n        label_dataset = f_out.create_dataset(\n            'labels',\n            shape=(0,),\n            maxshape=(None,),\n            dtype='int8',\n            chunks=True\n        )\n\n        for i in range(0, num_events, chunk_size):\n            start = i\n            end = min(i + chunk_size, num_events)\n            print(f\"-> Processing events {start} to {end-1}...\")\n\n            raw_chunk_data = dataset[start:end]\n            \n            chunk_images = []\n            X_chunk = raw_chunk_data[:, :-1]\n            y_chunk = raw_chunk_data[:, -1]\n            X_chunk_reshaped = X_chunk.reshape(-1, 700, 3)\n\n            for event in X_chunk_reshaped:\n                pt, eta, phi = event[:, 0], event[:, 1], event[:, 2]\n                jet_image, _, _ = np.histogram2d(\n                    eta, phi,\n                    bins = image_bins,      \n                    range = image_range, \n                    weights = pt\n                )\n                chunk_images.append(jet_image.T)\n            \n            chunk_images_np = np.array(chunk_images, dtype='float32')\n            current_size = image_dset.shape[0]\n            new_size = current_size + len(chunk_images_np)\n            \n            image_dataset.resize(new_size, axis=0)\n            image_dataset[current_size:] = chunk_images_np\n            \n            label_dataset.resize(new_size, axis=0)\n            label_dataset[current_size:] = y_chunk\n\n            del raw_chunk_data, chunk_images, chunk_images_np, X_chunk, y_chunk, X_chunk_reshaped\n            gc.collect()\n\nprint(f\"\\n✅ Processing complete. All data saved to '{output_path}'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:48:29.026051Z","iopub.execute_input":"2025-10-17T16:48:29.026645Z","iopub.status.idle":"2025-10-17T16:48:35.483119Z","shell.execute_reply.started":"2025-10-17T16:48:29.026610Z","shell.execute_reply":"2025-10-17T16:48:35.482051Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Processing data and saving progressively to 'processed_jet_images.h5'...\n-> Processing events 0 to 49999...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/565907462.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# --- CORRECTED HISTOGRAM CALL ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 jet_image, _, _ = np.histogram2d(\n\u001b[0m\u001b[1;32m     54\u001b[0m                     \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_bins\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# Use the number of bins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/twodim_base.py\u001b[0m in \u001b[0;36mhistogram2d\u001b[0;34m(x, y, bins, range, density, weights)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mxedges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myedges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxedges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myedges\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistogramdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36mhistogramdd\u001b[0;34m(sample, bins, range, density, weights)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;31m# Compute the bin number each sample falls into.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m     Ncount = tuple(\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0;31m# avoid np.digitize to work around gh-11022\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     Ncount = tuple(\n\u001b[1;32m   1028\u001b[0m         \u001b[0;31m# avoid np.digitize to work around gh-11022\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \"\"\"\n\u001b[0;32m-> 1400\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'searchsorted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mside\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"## Dataloader creation (with memory constraint --> slow)","metadata":{}},{"cell_type":"code","source":"class H5JetDataset(Dataset):\n    def __init__(self, h5_path, indices, max_pixel_value, is_train=True):\n        self.h5_path = h5_path\n        self.indices = indices\n        self.max_pixel_value = max_pixel_value\n        self.is_train = is_train\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        with h5py.File(self.h5_path, 'r') as f:\n            true_index = self.indices[idx]\n            image = f['jet_images'][true_index]\n            image = np.expand_dims(image, axis=0) / self.max_pixel_value\n            image_tensor = torch.from_numpy(image).float()\n            if self.is_train:\n                return (image_tensor,)\n            else:\n                label = f['labels'][true_index]\n                label_tensor = torch.tensor(label).float()\n                return image_tensor, label_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:09:18.268629Z","iopub.execute_input":"2025-10-18T10:09:18.269028Z","iopub.status.idle":"2025-10-18T10:09:18.278504Z","shell.execute_reply.started":"2025-10-18T10:09:18.269000Z","shell.execute_reply":"2025-10-18T10:09:18.277783Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"h5_path = '/kaggle/input/output/processed_jet_images.h5'\nwith h5py.File(h5_path, 'r') as f:\n    labels = f['labels'][:]\n    all_indices = np.arange(len(labels))\n    background_indices = all_indices[labels == 0]\n    signal_indices = all_indices[labels == 1]\n    sample_indices = np.random.choice(background_indices, 50000, replace=False)\n    sorted_sample_indices = np.sort(sample_indices)\n    max_pixel_value = f['jet_images'][sorted_sample_indices].max()\n\nprint(f\"Found {len(background_indices)} background and {len(signal_indices)} signal events.\")\nprint(f\"Data will be normalized by max pixel value: {max_pixel_value:.2f}\")\n\nbg_train_indices, bg_val_indices = train_test_split(\n    background_indices,\n    train_size=0.8,\n    random_state=42\n)\n\nval_indices = np.concatenate([bg_val_indices, signal_indices])\nnp.random.shuffle(val_indices)\n\ntrain_dataset = H5JetDataset(h5_path, \n                             indices=bg_train_indices, \n                             max_pixel_value=max_pixel_value, \n                             is_train=True)\n\nval_dataset = H5JetDataset(h5_path, \n                           indices=val_indices, \n                           max_pixel_value=max_pixel_value, \n                           is_train=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n\nprint(\"\\n--- Memory-Efficient DataLoaders Ready ---\")\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")\nprint(f\"Number of batches in train_loader: {len(train_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:41:10.518779Z","iopub.execute_input":"2025-10-18T10:41:10.519629Z","iopub.status.idle":"2025-10-18T10:54:31.933693Z","shell.execute_reply.started":"2025-10-18T10:41:10.519578Z","shell.execute_reply":"2025-10-18T10:54:31.932887Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Scanning HDF5 file to get indices and stats...\nFound 1000000 background and 100000 signal events.\nData will be normalized by max pixel value: 3672.09\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'# --- 3. Create Datasets and DataLoaders ---\\n# Create the training dataset (background only)\\ntrain_dataset = H5JetDataset(h5_path, \\n                             indices=bg_train_indices, \\n                             max_pixel_value=max_pixel_value, \\n                             is_train=True)\\n\\n# Create the validation dataset (mixed)\\nval_dataset = H5JetDataset(h5_path, \\n                           indices=val_indices, \\n                           max_pixel_value=max_pixel_value, \\n                           is_train=False)\\n\\n# Create the final DataLoaders\\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\\n\\nprint(\"\\n--- Memory-Efficient DataLoaders Ready ---\")\\nprint(f\"Number of training samples: {len(train_dataset)}\")\\nprint(f\"Number of validation samples: {len(val_dataset)}\")\\nprint(f\"Number of batches in train_loader: {len(train_loader)}\")'"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## Dataloader creation (small sample --> directly to ram --> fast)","metadata":{}},{"cell_type":"code","source":"def FastLoader(samples_used = 300000, dset =\"val\",file_path= None, train_indices=None, valid_indices=None, norm_value=None, batch_size=256, shuffle_set=False):\n    image_chunks = []\n    label_chunks = []\n     if dset == \"train\":\n        if train_indices is None:\n            raise ValueError(\"train_indices must be provided for dset='train'\")\n        sorted_indices = np.sort(train_indices)\n        load_labels = False\n    elif dset == \"val\":\n        if valid_indices is None:\n            raise ValueError(\"valid_indices must be provided for dset='val'\")\n        sorted_indices = np.sort(valid_indices)\n        load_labels = True\n    else:\n        raise ValueError(\"dset must be 'train' or 'val'\")\n    samples_used = samples_used\n    chunk_size = 20000\n    \n    with h5py.File(file_path, 'r') as f:\n        if 'jet_images' not in f:\n            raise KeyError(\"Dataset 'jet_images' not found in HDF5 file.\")\n        if load_labels and 'labels' not in f:\n             raise KeyError(\"Dataset 'labels' not found in HDF5 file (needed for validation).\")\n        img_dataset = f['jet_images']\n        label_dataset = f['labels'] if load_labels else None\n        for i in tqdm(range(0, samples_used, chunk_size), desc = f\"Loading {dset} data\"):\n            start = i\n            end = min(i + chunk_size, samples_used)\n            chunk_indices = sorted_indices[start:end]\n            image_chunks.append(f['jet_images'][chunk_indices])\n    \n    print(\"Concatenating chunks...\")\n    images_ram = np.concatenate(image_chunks)\n    images_ram = np.expand_dims(images_ram, axis=1) / norm_value\n    images_tensor = torch.from_numpy(images_ram).float()\n    if load_labels:\n        labels_ram = np.concatenate(label_chunks)\n        labels_tensor = torch.from_numpy(labels_ram).float()\n        dataset_fast = TensorDataset(images_tensor, labels_tensor)\n    else:\n        dataset_fast = TensorDataset(images_tensor)\n    loader = DataLoader(dataset_fast, batch_size=batch_size, shuffle=shuffle_set)\n    return loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = FastLoader(samples_used = 400000, dset=\"train\", file_path = jet_imgs_path, train_indices = bg_train_indices, norm_value = max_pixel_value, shuffle_set=True)\nval_loader = FastLoader(samples_used = len(val_indices), dset = \"val\", file_path = jet_imgs_path, valid_indices = val_indices, norm_value = max_pixel_value, shuffle_set=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Viusalizing Jet Images (normalized)**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\ndef plot_jet_images(loader, title = \"Sample Jet Images\"):\n    data_iter = iter(loader)\n    batch_data = next(data_iter)\n    if len(batch_data) == 1:\n        images, = batch_data \n        labels = None\n    else:\n        images, labels = batch_data\n        \n    fig, axes = plt.subplots(4, 4, figsize=(12, 11), constrained_layout=True)\n    fig.suptitle(title, fontsize=16)\n    mappable = None\n    for i in range(16):\n        if i >= len(images): \n            axes.flat[i].set_visible(False)\n            continue\n        ax = axes.flat[i]\n        img = images[i].squeeze().numpy()\n        im = ax.imshow(img, cmap='viridis', origin='lower')\n        if i == 0:\n            mappable = im\n        if i // 4 == 3:\n            ax.set_xlabel(\"Pseudorapidity (η)\")\n        if i % 4 == 0:\n            ax.set_ylabel(\"Azimuthal Angle (φ)\") \n        ax.set_xticks([])\n        ax.set_yticks([])\n        if labels is not None:\n            label_text = \"Signal\" if labels[i].item() == 1 else \"Background\"\n            ax.set_title(f\"Type: {label_text}\")\n    if mappable:\n        cbar = fig.colorbar(mappable, ax=axes.ravel().tolist(), shrink=0.8, pad=0.02)\n        cbar.set_label(\"Normalized Sum of $p_T$\")\n    plt.show()\n\n# --- Plotting the Samples ---\nprint(\"Displaying sample background jets from the training set...\")\nplot_jet_images(train_loader, title=\"Background Jets (for Training)\")\n\nprint(\"\\nDisplaying sample validation jets (mixed background and signal)...\")\nplot_jet_images(val_loader, title=\"Validation Jets\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T11:52:54.638345Z","iopub.execute_input":"2025-10-17T11:52:54.639160Z","iopub.status.idle":"2025-10-17T11:52:57.397049Z","shell.execute_reply.started":"2025-10-17T11:52:54.639134Z","shell.execute_reply":"2025-10-17T11:52:57.396246Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}