{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13293388,"sourceType":"datasetVersion","datasetId":8425451}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset , DataLoader\nfrom torch.utils.data import random_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport h5py\nimport optuna\nfrom optuna.pruners import MedianPruner\nimport torch.optim as optim\nimport os\n#import hdf5plugin\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:58:23.388001Z","iopub.execute_input":"2025-10-13T15:58:23.388223Z","iopub.status.idle":"2025-10-13T15:58:31.684006Z","shell.execute_reply.started":"2025-10-13T15:58:23.388200Z","shell.execute_reply":"2025-10-13T15:58:31.683237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(50)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:58:31.684705Z","iopub.execute_input":"2025-10-13T15:58:31.685265Z","iopub.status.idle":"2025-10-13T15:58:31.776696Z","shell.execute_reply.started":"2025-10-13T15:58:31.685244Z","shell.execute_reply":"2025-10-13T15:58:31.776130Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Preprocessing**\n","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/lhc-olympics-2020-ad-r-and-d/events_anomalydetection_v2.features.h5'\ndf = pd.read_hdf(file_path)\nprint(df.shape)\nprint(\"Memory in GB:\",sum(df.memory_usage(deep=True)) / (1024**3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:43:30.611032Z","iopub.execute_input":"2025-10-13T17:43:30.611565Z","iopub.status.idle":"2025-10-13T17:43:32.494335Z","shell.execute_reply.started":"2025-10-13T17:43:30.611543Z","shell.execute_reply":"2025-10-13T17:43:32.493545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Separating Background and Signal Data\nbg_data = df[df[\"label\"] == 0].drop(columns = \"label\")\nsignal_data = df[df[\"label\"] == 1].drop(columns = \"label\")\ntest_data = df.drop(columns = \"label\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:43:34.011206Z","iopub.execute_input":"2025-10-13T17:43:34.011821Z","iopub.status.idle":"2025-10-13T17:43:34.153737Z","shell.execute_reply.started":"2025-10-13T17:43:34.011798Z","shell.execute_reply":"2025-10-13T17:43:34.152772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(bg_data)\nbg_data_scaled = scaler.transform(bg_data)\nsignal_data_scaled = scaler.transform(signal_data)\ntest_data_scaled = scaler.transform(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:43:37.876397Z","iopub.execute_input":"2025-10-13T17:43:37.877014Z","iopub.status.idle":"2025-10-13T17:43:38.164818Z","shell.execute_reply.started":"2025-10-13T17:43:37.876991Z","shell.execute_reply":"2025-10-13T17:43:38.164041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FeatureDataset(Dataset):\n    def __init__(self,features):\n        self.features = torch.tensor(features,dtype = torch.float32)\n        \n    def __len__(self):\n        return len(self.features)\n\n    def __getitems__(self,idx):\n        x = self.features[idx]\n        return (x,x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:58:52.995930Z","iopub.execute_input":"2025-10-13T15:58:52.996663Z","iopub.status.idle":"2025-10-13T15:58:53.002499Z","shell.execute_reply.started":"2025-10-13T15:58:52.996630Z","shell.execute_reply":"2025-10-13T15:58:53.001538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = FeatureDataset(bg_data_scaled)\ntrain_dataset, val_dataset = random_split(train_dataset,[0.8,0.2])\ntest_dataset = FeatureDataset(test_data_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:43:41.400836Z","iopub.execute_input":"2025-10-13T17:43:41.401522Z","iopub.status.idle":"2025-10-13T17:43:41.524507Z","shell.execute_reply.started":"2025-10-13T17:43:41.401498Z","shell.execute_reply":"2025-10-13T17:43:41.523743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ***Linear Autoencoder***","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"class MyAE(nn.Module):\n    def __init__(self,num_hidden_layers,input_dim,latent_dim,trial):\n        super().__init__()\n        encoder_layers = []\n        current_dim = input_dim\n        for i in range(num_hidden_layers):\n            neurons_per_layer = trial.suggest_int(f\"layer_{i+1}\",8,128)\n            encoder_layers.append(nn.Linear(current_dim,neurons_per_layer))\n            encoder_layers.append(nn.ReLU())\n            current_dim = neurons_per_layer\n        encoder_layers.append(nn.Linear(current_dim,latent_dim))\n        self.encoder = nn.Sequential(*encoder_layers)\n\n        decoder_layers = []\n        encoder_children = list(self.encoder.children())\n        for layer in reversed(encoder_children):\n            if isinstance(layer, nn.Linear):\n                decoder_layers.append(nn.Linear(layer.out_features,layer.in_features))\n                decoder_layers.append(nn.ReLU())\n        decoder_layers.pop()\n        decoder_layers.append(nn.Sigmoid())\n        self.decoder = nn.Sequential(*decoder_layers)\n\n\n    def forward(self,x):\n        encoded_data = self.encoder(x)\n        reconstructed_data = self.decoder(encoded_data)\n        return reconstructed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T12:55:38.800971Z","iopub.execute_input":"2025-10-10T12:55:38.801550Z","iopub.status.idle":"2025-10-10T12:55:38.808820Z","shell.execute_reply.started":"2025-10-10T12:55:38.801524Z","shell.execute_reply":"2025-10-10T12:55:38.807929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    num_hidden_layers = trial.suggest_int(\"num_hidden_layers\",4,8)\n    batch_size = trial.suggest_categorical(\"batch_size\",[64,128,256,512])\n    latent_dim = trial.suggest_int(\"latent_dim\",2,8,step=2)\n    learning_rate = trial.suggest_float(\"learning_rate\",5e-5,1e-2,log=True)\n    weight_decay = trial.suggest_float(\"weight_decay\",1e-5,1e-3,log=True)\n    epochs = trial.suggest_int(\"epochs\",10,40,step = 5)\n    optimizer_name = trial.suggest_categorical(\"optimizer_name\",[\"Adam\",\"SGD\",\"RMSprop\"])\n\n    train_loader = DataLoader(train_dataset,batch_size = batch_size, shuffle = True, pin_memory = True)\n    test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, pin_memory = True)\n    val_loader = DataLoader(val_dataset,batch_size = batch_size, shuffle = False, pin_memory = True)\n\n    input_dim = 14\n    model = MyAE(num_hidden_layers,input_dim,latent_dim,trial)\n    model.to(device)\n    criterion = nn.MSELoss()\n    if optimizer_name == \"Adam\":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n    elif optimizer_name == \"RMSprop\":\n        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n    elif optimizer_name == \"SGD\":\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n\n    #Training Loop\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, targets in train_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            reconstructed_features = model(inputs)\n            loss = criterion(reconstructed_features, targets)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        epoch_loss = running_loss/len(train_loader)\n        print(f\"Epoch {epoch+1}/{epochs} : Loss - {epoch_loss:.4f}\")\n\n\n        #Validation loop\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n                reconstructed_features= model(inputs)\n                loss = criterion(reconstructed_features, targets)\n                val_loss += loss.item()\n        avg_loss = val_loss/len(val_loader)\n    \n        trial.report(avg_loss, epoch)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n        \n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T12:56:51.656402Z","iopub.execute_input":"2025-10-10T12:56:51.657010Z","iopub.status.idle":"2025-10-10T12:56:51.665927Z","shell.execute_reply.started":"2025-10-10T12:56:51.656973Z","shell.execute_reply":"2025-10-10T12:56:51.665142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pruner = MedianPruner()\nstudy = optuna.create_study(direction = \"minimize\")\nstudy.optimize(objective, n_trials = 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T12:57:04.166148Z","iopub.execute_input":"2025-10-10T12:57:04.166904Z","iopub.status.idle":"2025-10-10T13:02:27.580466Z","shell.execute_reply.started":"2025-10-10T12:57:04.166880Z","shell.execute_reply":"2025-10-10T13:02:27.579782Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(study.best_value)\nprint(study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:03:15.095405Z","iopub.execute_input":"2025-10-10T13:03:15.096203Z","iopub.status.idle":"2025-10-10T13:03:15.101053Z","shell.execute_reply.started":"2025-10-10T13:03:15.096178Z","shell.execute_reply":"2025-10-10T13:03:15.100203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Testing","metadata":{}},{"cell_type":"code","source":"#Hyper params\nbatch_size = 128\nlearning_rate = 0.0006609275044268213   \nweight_decay = 2.1608381475166164e-05   \nepochs = 30","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:41:24.489196Z","iopub.execute_input":"2025-10-10T13:41:24.490180Z","iopub.status.idle":"2025-10-10T13:41:24.494120Z","shell.execute_reply.started":"2025-10-10T13:41:24.490146Z","shell.execute_reply":"2025-10-10T13:41:24.493400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AE(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(14,112),\n            nn.ReLU(),\n            nn.Linear(112,125),\n            nn.ReLU(),\n            nn.Linear(125,39),\n            nn.ReLU(),\n            nn.Linear(39,25),\n            nn.ReLU(),\n            nn.Linear(25,8),\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(8,25),\n            nn.ReLU(),\n            nn.Linear(25,39),\n            nn.ReLU(),\n            nn.Linear(39,125),\n            nn.ReLU(),\n            nn.Linear(125,112),\n            nn.ReLU(),\n            nn.Linear(112,14),\n        )\n\n    def forward(self,x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\ntest_model = AE()\ntest_model.to(device)\noptimizer = optim.AdamW(test_model.parameters(), lr=learning_rate, weight_decay = weight_decay)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:41:29.311555Z","iopub.execute_input":"2025-10-10T13:41:29.312464Z","iopub.status.idle":"2025-10-10T13:41:29.322782Z","shell.execute_reply.started":"2025-10-10T13:41:29.312434Z","shell.execute_reply":"2025-10-10T13:41:29.322050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Training Loop\ncriterion = nn.MSELoss()\ntrain_loader = DataLoader(train_dataset,batch_size = batch_size, shuffle = True, pin_memory = True)\nfor epoch in range(epochs):\n    test_model.train()\n    running_loss = 0.0\n    for inputs, targets in train_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        reconstructed_features = test_model(inputs)\n        loss = criterion(reconstructed_features, targets)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    epoch_loss = running_loss/len(train_loader)\n    print(f\"Epoch {epoch+1}/{epochs} : Loss - {epoch_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:41:33.878911Z","iopub.execute_input":"2025-10-10T13:41:33.879647Z","iopub.status.idle":"2025-10-10T13:51:10.728967Z","shell.execute_reply.started":"2025-10-10T13:41:33.879613Z","shell.execute_reply":"2025-10-10T13:51:10.728149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Validation loop\ncriterion = nn.MSELoss(reduction = \"none\")\nval_loader = DataLoader(val_dataset,batch_size = batch_size, shuffle = False, pin_memory = True)\ntest_model.eval()\nval_loss = []\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        reconstructed_features= test_model(inputs)\n        loss = criterion(reconstructed_features, targets)\n        per_event_loss = torch.mean(loss, dim=1)\n        val_loss.extend(per_event_loss.cpu().numpy().tolist())\navg_loss = sum(val_loss)/len(val_dataset)\nmax_loss = max(val_loss)\nprint(f\"max loss is {max_loss}\")\nprint(f\"avg loss is {avg_loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:51:22.988121Z","iopub.execute_input":"2025-10-10T13:51:22.988852Z","iopub.status.idle":"2025-10-10T13:51:24.427731Z","shell.execute_reply.started":"2025-10-10T13:51:22.988827Z","shell.execute_reply":"2025-10-10T13:51:24.426957Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test loop\ncriterion = nn.MSELoss(reduction = \"none\")\ntest_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, pin_memory = True)\ntest_model.eval()\ntest_loss = []\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        reconstructed_features= test_model(inputs)\n        loss = criterion(reconstructed_features, targets)\n        per_event_loss = torch.mean(loss, dim=1)\n        test_loss.extend(per_event_loss.cpu().numpy().tolist())\n\ntest_loss = np.array(test_loss)\nall_labels = df[\"label\"].to_numpy().astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:51:40.307722Z","iopub.execute_input":"2025-10-10T13:51:40.308404Z","iopub.status.idle":"2025-10-10T13:51:47.854872Z","shell.execute_reply.started":"2025-10-10T13:51:40.308377Z","shell.execute_reply":"2025-10-10T13:51:47.854232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:51:58.905458Z","iopub.execute_input":"2025-10-10T13:51:58.906185Z","iopub.status.idle":"2025-10-10T13:51:58.911851Z","shell.execute_reply.started":"2025-10-10T13:51:58.906160Z","shell.execute_reply":"2025-10-10T13:51:58.910712Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ROC Curve (Threshold analysis)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(all_labels, test_loss)\nauc_score = roc_auc_score(all_labels, test_loss)\n\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n# Plot the optimal point\nplt.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', label=f'Optimal Threshold = {optimal_threshold:.4f}')\nplt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()\n\nprint(f\"Optimal threshold based on ROC curve: {optimal_threshold:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:52:17.682305Z","iopub.execute_input":"2025-10-10T13:52:17.682868Z","iopub.status.idle":"2025-10-10T13:52:18.908963Z","shell.execute_reply.started":"2025-10-10T13:52:17.682844Z","shell.execute_reply":"2025-10-10T13:52:18.908157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Anomaly Prediction\npreds = (test_loss > optimal_threshold).astype(int)\ncorrect_pred = (preds == all_labels)\naccuracy = correct_pred.sum()/len(all_labels)\nprint(accuracy)\nprint(preds)\nprint(all_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T13:52:43.743342Z","iopub.execute_input":"2025-10-10T13:52:43.743946Z","iopub.status.idle":"2025-10-10T13:52:43.756914Z","shell.execute_reply.started":"2025-10-10T13:52:43.743911Z","shell.execute_reply":"2025-10-10T13:52:43.756181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ***Variational Autoencoder***","metadata":{}},{"cell_type":"markdown","source":"## Hyperparam Tuning","metadata":{}},{"cell_type":"code","source":"#Model Architecture\nclass MyVAE(nn.Module):\n    def __init__(self,num_hidden_layers,input_dim,latent_dim,trial):\n        super().__init__()\n        encoder_layers = []\n        current_dim = input_dim\n        for i in range(num_hidden_layers):\n            neurons_per_layer = trial.suggest_int(f\"layer_{i+1}\",8,128)\n            encoder_layers.append(nn.Linear(current_dim,neurons_per_layer))\n            encoder_layers.append(nn.ReLU())\n            current_dim = neurons_per_layer\n        self.encoder = nn.Sequential(*encoder_layers)\n\n        self.fc_mu = nn.Linear(current_dim, latent_dim)\n        self.fc_logvar = nn.Linear(current_dim, latent_dim)\n\n        decoder_layers = []\n        decoder_layers.append(nn.Linear(latent_dim, current_dim))\n        encoder_children = list(self.encoder.children())\n        for layer in reversed(encoder_children):\n            if isinstance(layer, nn.Linear):\n                decoder_layers.append(nn.Linear(layer.out_features,layer.in_features))\n                decoder_layers.append(nn.ReLU())\n        decoder_layers.pop()\n        self.decoder = nn.Sequential(*decoder_layers)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        return self.fc_mu(h), self.fc_logvar(h)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        return self.decoder(z)\n\n    def forward(self, x):\n        mu, logvar = self.encode(x.view(-1, 14))\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\n\ndef loss_function(recon_x, x, mu, logvar, beta=1.0):\n    recon_loss = nn.functional.mse_loss(recon_x, x.view(-1, 14), reduction='sum')/x.shape[0]\n    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim = 1).mean()\n    return recon_loss + (beta * kl_div)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:24:52.187537Z","iopub.execute_input":"2025-10-13T17:24:52.188252Z","iopub.status.idle":"2025-10-13T17:24:52.197169Z","shell.execute_reply.started":"2025-10-13T17:24:52.188223Z","shell.execute_reply":"2025-10-13T17:24:52.196419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    num_hidden_layers = trial.suggest_int(\"num_hidden_layers\",4,8)\n    batch_size = trial.suggest_categorical(\"batch_size\",[64,128,256,512])\n    latent_dim = trial.suggest_int(\"latent_dim\",2,8,step=2)\n    learning_rate = trial.suggest_float(\"learning_rate\",5e-5,5e-3,log=True)\n    weight_decay = trial.suggest_float(\"weight_decay\",1e-5,1e-3,log=True)\n    epochs = trial.suggest_int(\"epochs\",10,40,step = 5)\n    beta = trial.suggest_float(\"beta\",0.1,1.0,log=True)\n    optimizer_name = trial.suggest_categorical(\"optimizer_name\",[\"AdamW\",\"Adam\"])\n\n    train_loader = DataLoader(train_dataset,batch_size = batch_size, shuffle = True, pin_memory = True)\n    test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, pin_memory = True)\n    val_loader = DataLoader(val_dataset,batch_size = batch_size, shuffle = False, pin_memory = True)\n\n    input_dim = 14\n    model = MyVAE(num_hidden_layers,input_dim,latent_dim,trial)\n    model.to(device)\n    if optimizer_name == \"Adam\":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n    elif optimizer_name == \"RMSprop\":\n        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n    elif optimizer_name == \"AdamW\":\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n\n    #Training Loop\n    for epoch in range(epochs):\n        model.train()\n        running_total_loss = 0.0\n        running_recon_loss = 0.0\n        running_kld_loss = 0.0\n        for inputs, targets in train_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            reconstructed_features, mu, logvar = model(inputs)\n            recon_loss = nn.functional.mse_loss(reconstructed_features, targets, reduction='sum')\n            kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n            total_loss = recon_loss + (beta * kl_div)\n            #loss = loss_function(reconstructed_features, targets, mu, logvar)\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n            running_total_loss += total_loss.item()\n            running_recon_loss += recon_loss.item()\n            running_kld_loss += kl_div.item()\n        avg_total_loss = running_total_loss / len(train_loader.dataset)\n        avg_recon_loss = running_recon_loss / len(train_loader.dataset)\n        avg_kld_loss = running_kld_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{epochs} | \"\n          f\"Total Loss: {avg_total_loss:.4f} | \"\n          f\"Recon Loss: {avg_recon_loss:.4f} | \"\n          f\"KLD: {avg_kld_loss:.4f}\")\n\n\n        #Validation loop\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n                reconstructed_features, mu, logvar = model(inputs)\n                loss = loss_function(reconstructed_features, targets, mu, logvar,beta)\n                val_loss += loss.item()\n        avg_loss = val_loss/len(val_loader)\n    \n        trial.report(avg_loss, epoch)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n        \n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:24:55.839332Z","iopub.execute_input":"2025-10-13T17:24:55.840035Z","iopub.status.idle":"2025-10-13T17:24:55.851601Z","shell.execute_reply.started":"2025-10-13T17:24:55.840011Z","shell.execute_reply":"2025-10-13T17:24:55.850719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pruner = MedianPruner()\nstudy = optuna.create_study(direction = \"minimize\")\nstudy.optimize(objective, n_trials = 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:25:02.914728Z","iopub.execute_input":"2025-10-13T17:25:02.915074Z","iopub.status.idle":"2025-10-13T17:36:34.532112Z","shell.execute_reply.started":"2025-10-13T17:25:02.915054Z","shell.execute_reply":"2025-10-13T17:36:34.531265Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(study.best_value)\nprint(study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:37:09.958834Z","iopub.execute_input":"2025-10-13T17:37:09.959148Z","iopub.status.idle":"2025-10-13T17:37:09.963736Z","shell.execute_reply.started":"2025-10-13T17:37:09.959126Z","shell.execute_reply":"2025-10-13T17:37:09.963027Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Testing","metadata":{}},{"cell_type":"code","source":"#Hyperparams\nbeta = 0.14134371590127678\nbatch_size = 256\nlearning_rate = 0.003290904745561437\nweight_decay = 1.4399478254951877e-05","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:01:28.833572Z","iopub.execute_input":"2025-10-13T17:01:28.833838Z","iopub.status.idle":"2025-10-13T17:01:28.837611Z","shell.execute_reply.started":"2025-10-13T17:01:28.833817Z","shell.execute_reply":"2025-10-13T17:01:28.836930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(14,44),\n            nn.ReLU(),\n            nn.Linear(44, 127),\n            nn.ReLU(),\n            nn.Linear(127, 60),\n            nn.ReLU(),\n            nn.Linear(60, 97),\n            nn.ReLU()\n        )\n        self.fc_mu = nn.Linear(97, 8) # for mean\n        self.fc_logvar = nn.Linear(97, 8) # for log variance\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(8, 97),\n            nn.ReLU(),\n            nn.Linear(97,60),\n            nn.ReLU(),\n            nn.Linear(60, 127),\n            nn.ReLU(),\n            nn.Linear(127, 44),\n            nn.ReLU(),\n            nn.Linear(44, 14)\n        )\n\n    def encode(self, x):\n        h = self.encoder(x)\n        return self.fc_mu(h), self.fc_logvar(h)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        return self.decoder(z)\n\n    def forward(self, x):\n        mu, logvar = self.encode(x.view(-1, 14))\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\ndef loss_function(recon_x, x, mu, logvar, beta=1.0):\n    recon_loss = nn.functional.mse_loss(recon_x, x.view(-1, 14), reduction='sum')\n    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + (beta * kl_div)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:00:23.278219Z","iopub.execute_input":"2025-10-13T18:00:23.278888Z","iopub.status.idle":"2025-10-13T18:00:23.286841Z","shell.execute_reply.started":"2025-10-13T18:00:23.278865Z","shell.execute_reply":"2025-10-13T18:00:23.286007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_dim = 14\nvae_model = VAE()\nvae_model.to(device)\noptimizer = optim.AdamW(vae_model.parameters(), lr=learning_rate, weight_decay = weight_decay)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:00:35.870007Z","iopub.execute_input":"2025-10-13T18:00:35.870640Z","iopub.status.idle":"2025-10-13T18:00:35.878431Z","shell.execute_reply.started":"2025-10-13T18:00:35.870616Z","shell.execute_reply":"2025-10-13T18:00:35.877759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Training Loop\nmodel_save_path = 'best_model.pth'\npatience = 10 \nbest_val_loss = float('inf')\npatience_counter = 0\nanneal_epochs = 10\nepochs = 50\n\ntrain_loader = DataLoader(train_dataset,batch_size = batch_size, shuffle = True, pin_memory = True)\nval_loader = DataLoader(val_dataset,batch_size = batch_size, shuffle = False, pin_memory = True)\n\nfor epoch in range(epochs):\n    print(f\"--- Epoch {epoch+1}/{epochs} ---\")\n    vae_model.train()\n    running_total_loss = 0.0\n    running_recon_loss = 0.0\n    running_kld_loss = 0.0\n    kl_weight = min(1.0, epoch/anneal_epochs) * beta    \n    for inputs, targets in train_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        reconstructed_features, mu, logvar = vae_model(inputs)\n        recon_loss = nn.functional.mse_loss(reconstructed_features, targets, reduction='sum')\n        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n        total_loss = recon_loss + (kl_weight * kl_div)\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n        running_total_loss += total_loss.item()\n        running_recon_loss += recon_loss.item()\n        running_kld_loss += kl_div.item()\n    avg_total_loss = running_total_loss / len(train_loader.dataset)\n    avg_recon_loss = running_recon_loss / len(train_loader.dataset)\n    avg_kld_loss = running_kld_loss / len(train_loader.dataset)\n    print(f\"Epoch {epoch+1}/{epochs} | \"\n      f\"Total Loss: {avg_total_loss:.4f} | \"\n      f\"Recon Loss: {avg_recon_loss:.4f} | \"\n      f\"KLD: {avg_kld_loss:.4f}\")\n\n     # --- Validation Phase ---\n    vae_model.eval()\n    current_val_loss = 0.0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            reconstructed_features, mu, logvar = vae_model(inputs)\n            loss = loss_function(reconstructed_features, targets, mu, logvar,beta=beta)\n            current_val_loss += loss.item()    \n    avg_val_loss = current_val_loss / len(val_loader.dataset)\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n\n    # --- Early Stopping Logic ---\n    if avg_val_loss < best_val_loss:\n        print(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model...\")\n        best_val_loss = avg_val_loss\n        torch.save(vae_model.state_dict(), model_save_path)\n        patience_counter = 0 # Reset patience\n    else:\n        patience_counter += 1\n        print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n        if patience_counter >= patience:\n            print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n            break\n\nprint(\"\\n--- Training finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:00:38.599415Z","iopub.execute_input":"2025-10-13T18:00:38.599689Z","iopub.status.idle":"2025-10-13T18:11:24.833790Z","shell.execute_reply.started":"2025-10-13T18:00:38.599669Z","shell.execute_reply":"2025-10-13T18:11:24.833182Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test Loop\nfrom sklearn.metrics import roc_auc_score\nvae_model = VAE()\nvae_model.to(device)\nvae_model.load_state_dict(torch.load('best_model.pth'))\nvae_model.eval()\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nall_recon_errors = []\nall_klds = []\nall_total_losses = []\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        reconstructed_features, mu, logvar = vae_model(inputs)\n        recon_error_per_event = torch.sum(nn.functional.mse_loss(reconstructed_features, targets, reduction='none'), dim=1)\n        kld_per_event = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n        total_loss_per_event = recon_error_per_event + (beta * kld_per_event)\n\n        all_recon_errors.extend(recon_error_per_event.cpu().numpy().tolist())\n        all_klds.extend(kld_per_event.cpu().numpy().tolist())\n        all_total_losses.extend(total_loss_per_event.cpu().numpy().tolist())\n        \nall_labels = df[\"label\"].to_numpy().astype(int)\nall_recon_errors = np.array(all_recon_errors)\nall_klds = np.array(all_klds)\nall_total_losses = np.array(all_total_losses)\n\nauc_recon = roc_auc_score(all_labels, all_recon_errors)\nauc_kld = roc_auc_score(all_labels, all_klds)\nauc_total = roc_auc_score(all_labels, all_total_losses)\n\nprint(f\"AUC using Reconstruction Error: {auc_recon:.4f}\")\nprint(f\"AUC using KL Divergence:        {auc_kld:.4f}\")\nprint(f\"AUC using Total VAE Loss:       {auc_total:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:15:08.335246Z","iopub.execute_input":"2025-10-13T18:15:08.335739Z","iopub.status.idle":"2025-10-13T18:15:19.544746Z","shell.execute_reply.started":"2025-10-13T18:15:08.335716Z","shell.execute_reply":"2025-10-13T18:15:19.543942Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ROC Curve (Threshold Analysis)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(all_labels, all_total_losses)\nauc_score = auc_total\n\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\n\n# Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n# Plot the optimal point\nplt.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', label=f'Optimal Threshold = {optimal_threshold:.4f}')\nplt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()\n\nprint(f\"Optimal threshold based on ROC curve: {optimal_threshold:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:15:38.025062Z","iopub.execute_input":"2025-10-13T18:15:38.025553Z","iopub.status.idle":"2025-10-13T18:15:38.655166Z","shell.execute_reply.started":"2025-10-13T18:15:38.025531Z","shell.execute_reply":"2025-10-13T18:15:38.654277Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Anomaly Prediction\npreds = (all_total_losses > optimal_threshold).astype(int)\ncorrect_pred = (preds == all_labels)\naccuracy = correct_pred.sum()/len(all_labels)\nprint(accuracy)\nprint(preds)\nprint(all_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:15:43.736745Z","iopub.execute_input":"2025-10-13T18:15:43.737490Z","iopub.status.idle":"2025-10-13T18:15:43.749877Z","shell.execute_reply.started":"2025-10-13T18:15:43.737457Z","shell.execute_reply":"2025-10-13T18:15:43.749249Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}